# AI IDE 本地多模型集成 - 实现提示词

> 本文档包含7个核心任务的提示词，可直接复制给开发AI（Claude、GPT-4等）使用。
> 
> 使用方法：按顺序将各任务提示词发送给AI，逐步完成系统开发。

---

## 模型配置概览

| 用途 | 推荐模型 | 参数量 | Q4存储 | 上下文窗口 |
|------|----------|--------|--------|------------|
| 索引生成 | Llama 3.2 3B | 3B | ~2GB | 128K |
| 深度分析 | Phi-4-mini | 3.8B | ~3GB | 128K |
| 上下文总结 | Qwen3-4B | 4B | ~3GB | 32K+ |
| **总计** | — | — | **~8GB** | — |

---

## 功能模式定义

| 模式 | 启用的模型 | 功能描述 |
|------|-----------|----------|
| 汇总 | Qwen3-4B | 总结当前上下文 |
| 编译 | Llama 3.2 | 生成项目索引 |
| 规划 | Llama 3.2 + Phi-4-mini | 索引+分析+分块处理 |
| 查询 | 云端API | 不使用本地模型 |
| 默认 | 自动判断 | 根据上下文大小自动选择 |

---

# 任务1：系统架构设计

## 任务描述

```markdown
# 任务：为 Electron IDE 设计本地多模型AI系统

## 项目背景
我正在开发一个基于 Electron 的 AI IDE，核心功能是：
- 项目目录智能索引
- 代码上下文分析与总结
- 项目进度自动汇总

## 技术约束
- 运行环境：Electron（主进程 Node.js，渲染进程 Web）
- 本地模型数量：3个
  - 模型A（~3B参数）：索引生成，擅长结构化输出
  - 模型B（~4B参数，128K上下文）：深度分析
  - 模型C（~4B参数）：上下文总结
- 总存储预算：8-10GB（量化后）

## 功能模式
| 模式 | 使用模型 | 用途 |
|------|----------|------|
| 汇总 | 模型C | 总结当前打开的代码上下文 |
| 编译 | 模型A | 扫描项目生成结构化索引 |
| 规划 | 模型A+B | 索引 → 分析 → 生成开发计划 |
| 查询 | 云端API | 复杂问题转云端处理 |
| 默认 | 自动判断 | 根据输入大小智能路由 |

## 请输出
1. **Electron 架构设计**：
   - 主进程职责（模型调用、文件系统）
   - 渲染进程职责（UI、用户交互）
   - IPC 通信协议

2. **模型运行时选型建议**：
   - 比较 Ollama / llama.cpp / LocalAI / vLLM
   - 推荐 Electron 场景下最优方案
   - 给出理由

3. **系统架构图**（ASCII 或 Mermaid）

4. **关键技术决策**：
   - 模型如何打包/下载（内置还是用户自行安装？）
   - 模型进程如何与 Electron 主进程隔离
   - 内存管理策略
```

---

# 任务2：主进程模型调度器

## 任务描述

```markdown
# 任务：实现 Electron 主进程的模型调度器

## 背景
Electron 主进程负责与本地模型通信，需要根据任务类型智能调度。

## TypeScript 接口定义
```typescript
interface ModelConfig {
  id: string;
  name: string;
  size: string;
  contextWindow: number;
  capabilities: ('index' | 'analysis' | 'summarize')[];
}

interface SchedulerOptions {
  taskType: 'summarize' | 'index' | 'plan' | 'query' | 'auto';
  content: string;
  contextSize: number; // tokens
}

interface SchedulerResult {
  selectedModel: string;
  requiresChunking: boolean;
  chunks?: string[];
}
```

## 调度逻辑
```
auto 模式：
  - contextSize < 10K → 使用模型C（总结）
  - contextSize 10K-100K → 使用模型B（分析）
  - contextSize > 100K → 分块 + 模型A索引 → 模型B合并

plan 模式：
  - 先用模型A生成索引
  - 再用模型B基于索引做规划
  - 大项目自动分块处理
```

## 请输出
1. ModelScheduler 类完整实现（TypeScript）
2. 与模型运行时的通信适配层（抽象接口，支持多种后端）
3. 错误处理：模型未安装、内存不足、超时等
4. 单元测试用例
```

---

# 任务3：分块与索引策略

## 任务描述

```markdown
# 任务：设计大项目的分块索引和合并策略

## 问题
当项目超过 100 个文件，总 token 数超过 100K 时，无法单次处理。

## 约束
- 单次处理目标：30-50K tokens
- 需要保持代码语义完整性
- 需要能处理跨文件依赖关系

## 请设计

### 1. 分块算法
- 按目录结构切分？按文件类型？按依赖图？
- 单个 chunk 的边界如何确定？
- 如何处理"半个函数"的情况？

### 2. 索引数据结构
```typescript
interface ProjectIndex {
  meta: {
    projectName: string;
    indexedAt: Date;
    fileCount: number;
  };
  structure: DirectoryNode[];
  modules: ModuleInfo[];
  dependencies: DependencyGraph;
  entryPoints: string[];
}
```

### 3. 合并策略
- 多个 chunk 的索引如何合并为一个完整索引？
- 如何检测和去重重复信息？

### 4. 增量更新
- 用户修改一个文件后，如何增量更新索引？
- 哪些操作需要触发全量重建？

## 请输出
1. ChunkProcessor 类实现
2. 索引 JSON Schema 完整定义
3. 合并算法伪代码
4. 增量更新流程图
```

---

# 任务4：渲染进程 UI 组件

## 任务描述

```markdown
# 任务：实现 Electron 渲染进程的控制面板

## 技术栈
- React（或 Vue，根据项目选择）
- TailwindCSS 或等效样式方案

## 需要的 UI 组件

### 1. 模式选择器
- 5个选项：汇总 / 编译 / 规划 / 查询 / 自动
- 当前选中状态
- 快捷键支持

### 2. 进度指示器
- 显示当前任务进度
- 模型加载状态
- 分块处理进度（N/M chunks）

### 3. 结果展示面板
- 索引结果（树形结构）
- 分析报告（Markdown 渲染）
- 错误信息

### 4. 设置面板
- 模型运行时配置（地址、端口）
- 模型下载管理
- 性能参数调整

## 请输出
1. 组件结构设计
2. 关键组件的 React/Vue 代码
3. 与主进程的 IPC 通信代码
4. 样式建议（不要过度设计，保持简洁）
```

---

# 任务5：IPC 通信协议

## 任务描述

```markdown
# 任务：定义 Electron IPC 通信协议

## 背景
Electron 主进程和渲染进程需要通过 IPC 通信，完成模型调用。

## 需要的 IPC 通道

### 渲染进程 → 主进程
- `model:summarize` - 请求上下文总结
- `model:index` - 请求项目索引
- `model:plan` - 请求规划分析
- `model:query` - 请求云端查询
- `model:status` - 查询模型状态

### 主进程 → 渲染进程
- `model:progress` - 任务进度更新
- `model:result` - 返回结果
- `model:error` - 错误通知

## 请输出
1. 完整的 IPC 类型定义
2. 主进程处理函数实现
3. 渲染进程调用封装
4. 流式响应的处理方案（SSE/WebSocket/逐块返回）
```

---

# 任务6：模型运行时集成

## 任务描述

```markdown
# 任务：实现模型运行时客户端

## 背景
需要封装一个通用的模型客户端，支持多种后端（Ollama/llama.cpp/LocalAI）。

## 需求
1. 统一的 API 接口，屏蔽后端差异
2. 支持流式输出
3. 支持上下文管理
4. 支持健康检查

## 目标模型
```bash
# Ollama 拉取命令
ollama pull llama3.2:3b
ollama pull phi:3.8b
ollama pull qwen3:4b
```

## 请输出
1. ModelClient 抽象类/接口
2. OllamaClient 实现
3. llama.cpp 客户端实现（可选）
4. 健康检查和自动重连逻辑
5. 超时和重试策略
```

---

# 任务7：完整系统集成

## 任务描述

```markdown
# 任务：整合所有组件为可用的 IDE 功能

## 已有组件
1. ModelScheduler - 模型调度器
2. ModelClient - 模型客户端
3. ChunkProcessor - 分块处理器
4. UI 组件 - 控制面板

## 需要实现
1. **入口集成**：
   - 在 IDE 中添加触发入口（命令面板、右键菜单）
   - 快捷键绑定

2. **结果展示**：
   - 侧边栏面板
   - 底部状态栏
   - 弹窗/通知

3. **缓存策略**：
   - 索引结果缓存到磁盘
   - 增量更新检测
   - 缓存过期策略

4. **配置持久化**：
   - 用户偏好保存
   - 模型路径配置

## 请输出
1. 完整的目录结构
2. 入口文件和初始化流程
3. 配置文件示例
4. 用户使用文档（中文）
```

---

# 使用说明

## 推荐执行顺序

1. **任务1** → 获得整体架构设计
2. **任务5** → 定义 IPC 协议（前后端通信基础）
3. **任务2** → 实现调度器核心逻辑
4. **任务3** → 实现分块处理
5. **任务6** → 实现模型客户端
6. **任务4** → 实现 UI 组件
7. **任务7** → 系统集成

## 给 AI 的附加说明

在执行每个任务时，告诉 AI：

> 这是一个 Electron 项目，主进程使用 Node.js/TypeScript，渲染进程使用 React。请确保代码：
> 1. 类型安全（完整 TypeScript 类型）
> 2. 错误处理完善
> 3. 包含必要注释
> 4. 遵循现有项目代码风格

---

# 附录：模型快速部署命令

## Ollama 安装

```bash
# Windows (PowerShell)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

## 模型拉取

```bash
# 索引模型
ollama pull llama3.2:3b

# 分析模型
ollama pull phi:3.8b

# 总结模型
ollama pull qwen3:4b
```

## 验证安装

```bash
ollama list
ollama run llama3.2:3b "Hello"
```

---

# 文件位置说明

本文件保存于：`.sisyphus/drafts/实现提示词.md`

请复制到目标目录：
```
D:\yoka open ide\本地模型集成方案\实现提示词.md
```

对应的工作计划文件：
```
.sisyphus/plans/本地模型集成方案.md
```

请复制到：
```
D:\yoka open ide\本地模型集成方案\完整工作计划.md
```
